# -*- coding: utf-8 -*-
"""Banking Churn Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/129M8EwK7c3m062b1s9Rrre8dGX5OQb24
"""

#Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

#Load dataset
data_raw = pd.read_csv('/content/Churn_Modelling.csv')
data = data_raw.drop(data_raw.columns[0], axis=1)
data.head(10)

data.info()  #Check for missing values and data types

data.describe()  #Summary statistics

#Count the occurrences of churn
churn = pd.DataFrame(data['Exited'].replace({1: 'Yes', 0: 'No'}))
churn.head()

#Create a bar chart
sns.countplot(x='Exited', data=churn)

#Add titles and labels
plt.title('Count of Churn Customer')
plt.xlabel('Exited?')
plt.ylabel('Count')

#Drop customer ID and name
data1 = data.drop(columns=['CustomerId', 'Surname'])

#Converts categorical variables to dummy variables
data2 = pd.get_dummies(data1, drop_first=True)
data2.head()

#Check correlation
corr_matrix = data2.corr()
sns.heatmap(corr_matrix, annot=True, fmt='.2f')
plt.show()

#Split the data into training and test sets
X = data2.drop('Exited', axis=1)  #Features (independent variables)
y = data2['Exited']  #Target (dependent variable)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#Random Forest Classifier
model = RandomForestClassifier(n_estimators=200, random_state=42)
model.fit(X_train, y_train)  # Train the model

#Predict on test data
y_pred = model.predict(X_test)

#Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

#Classification Report
print(classification_report(y_test, y_pred))

#Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

#Hyperparameter tuning to improve model's performance
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

#Best parameters
print("Best Parameters:", grid_search.best_params_)

#Create visualization for feature importances
feature_names = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Geography_Germany', 'Geography_Spain', 'Gender_Male']  # Replace with actual names
importances = pd.Series(model.feature_importances_, index=feature_names)

#Plot
importances.plot(kind='barh')
plt.title('Feature Importances')
plt.show()